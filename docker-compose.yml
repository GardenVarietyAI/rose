services:
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server-b7464
    env_file:
      - .env.docker
    ports:
      - "8080:8080"
    volumes:
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
      - ./llama-server/entrypoint.sh:/entrypoint.sh:ro
    entrypoint: [ "/entrypoint.sh" ]
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  rose-server:
    build: .
    env_file:
      - .env.docker
    ports:
      - "8004:8004"
    volumes:
      - .:/app:rw
    depends_on:
      llama-server:
        condition: service_healthy
    environment:
      - SETTINGS_ENV_FILE=.env.docker
      - OPENAI_BASE_URL=http://llama-server:8080/v1
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8004/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  esbuild:
    build:
      context: .
      dockerfile: esbuild/Dockerfile
    profiles: [ "tools" ]
    working_dir: /app
    volumes:
      - .:/app
