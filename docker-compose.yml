services:
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server-b7464
    ports:
      - "8080:8080"
    volumes:
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
      - ./llama-server/entrypoint.sh:/entrypoint.sh:ro
    entrypoint: ["/bin/sh", "/entrypoint.sh"]
    command: >
      --host 0.0.0.0
      --port 8080
      -ngl -1
      -c 2048
      -n 512
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  rose-server:
    build: .
    ports:
      - "8004:8004"
    volumes:
      - .:/app
      - ./vendor/nltk_data:/usr/local/nltk_data:ro
    depends_on:
      llama-server:
        condition: service_healthy
    environment:
      - SETTINGS_ENV_FILE=.env.docker
      - OPENAI_BASE_URL=http://llama-server:8080/v1
      - NLTK_DATA=/usr/local/nltk_data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
