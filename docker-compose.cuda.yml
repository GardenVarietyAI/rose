services:
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda-b7464
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              count: 1
              driver: nvidia
