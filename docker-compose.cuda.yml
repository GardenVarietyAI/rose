services:
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda-b7464
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              count: 1
              driver: nvidia
    volumes:
      - ./llama-server/entrypoint-cuda.sh:/entrypoint-cuda.sh:ro
    entrypoint: ["/entrypoint-cuda.sh"]
