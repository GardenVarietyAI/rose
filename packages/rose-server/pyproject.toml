[project]
name = "rose-server"
version = "0.1.5"
description = "ROSE Server - Run your own LLM server"
requires-python = ">=3.12,<4.0"
authors = [{ name = "Garden Variety AI" }]
license = { text = "MIT" }
keywords = ["llm", "ai", "openai", "inference", "server"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3.12",
]
dependencies = [
    "fastapi>=0.110.0",
    "uvicorn>=0.27.1",
    "pydantic~=2.11.7",
    "llama-cpp-python>=0.3.5",
    "numpy>=2.2.1",
    "sqlalchemy>=2.0.0",
    "sqlmodel>=0.0.24",
    "sse-starlette>=2.1.3",
    "aiosqlite>=0.21.0",
    "sqlite-vec>=0.1.6",
    "typer>=0.16.0",
    "greenlet>=3.2.2",
    "tokenizers>=0.21.2",
    "jinja2>=3.1.6",
    "symspellpy>=6.9.0",
    "rake-nltk>=1.0.6",
    "yoyo-migrations>=9.0.0",
]

[project.optional-dependencies]
cuda = [
    "llama-cpp-python[server]>=0.3.5",
]

[project.scripts]
rose-server = "rose_server.main:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
packages = ["src/rose_server"]
