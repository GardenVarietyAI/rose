[project]
name = "rose-server"
version = "0.1.4"
description = "ROSE Server - Run your own LLM server"
readme = "README.md"
requires-python = ">=3.12,<4.0"
authors = [{ name = "Garden Variety AI" }]
license = { text = "MIT" }
keywords = ["llm", "ai", "openai", "inference", "server"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3.12",
]
dependencies = [
    "fastapi>=0.110.0",
    "uvicorn>=0.27.1",
    "pydantic~=2.11.7",
    "numpy>=1.26.4",
    "pydantic-settings>=2.9.1",
    "openai>=1.88.0",
    "openai-agents>=0.0.19",
    "sqlalchemy>=2.0.0",
    "sqlmodel>=0.0.24",
    "sse-starlette>=2.1.3",
    "aiosqlite>=0.21.0",
    "greenlet>=3.2.2",
    "aiofiles>=24.1.0",
    "python-multipart>=0.0.18",
    "tokenizers>=0.21.2",
    "sqlite-vec>=0.1.6",
    "chonkie[tokenizers]~=1.1.2",
    "pypdf>=6.0.0",
    "jinja2>=3.1.6",
    "safetensors>=0.6.2",
    "typer>=0.9.0",
]

[project.scripts]
rose-server = "rose_server.main:main"

[build-system]
requires = ["maturin>=1.9.0"]
build-backend = "maturin"

[tool.maturin]
bindings = "pyo3"
module-name = "rose_server._inference"
manifest-path = "../rose-inference-rs/Cargo.toml"
python-source = "src"
